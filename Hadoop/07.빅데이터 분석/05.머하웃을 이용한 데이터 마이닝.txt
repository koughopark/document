- 머하웃을 이용한 데이터 마이닝은 빅데이터에서 사람이 인지하기 어려운 패턴을 발견하고 통찰력을 갖게 만든다.
- 이를 통해 앞으로 발생할 일들을 예측하면서 신속한 의사결정을 지원하게 된다.
- 지금까지 수집, 적재, 처리, 분석한 스마트카의 데이터셋을 가지고 머하웃의 세 가지 마이닝 기법인 추천, 분류, 군집 기능을 활용해 좀더 가치 있는 데이터 분석을 수행

1. 스마트카 차량용품 추천
	# 데이터 마이닝의 추천에 사용될 데이터셋은 "스마트카 차량용품 구매 이력" 정보로서 Managed 영역에 있는 Managed_SmartCar_Item_BuyList_Info 테이블에 약 10만 건의 데이터가 적재돼 있다.
	# 이 가운데 추천에 필요한 항목은 3개의 필드로서 차량고유번호(car_number), 구매용품아이템코드(item), 사용평가점수(score)다.

	# 파일럿 프로젝트의 "스마트카 차량용품 구매 이력" 데이터로 머하웃의 추천기 모델을 만들고 실행

	# "스마트카 용품 구매 이력" 데이터를 머하웃의 추천기에서 사용 가능한 형식으로 재구성한 파일을 만들어야 한다.
	# 휴 -> Hive Editor 에서 아래 QL을 실행
========[내부코드]===========
insert overwrite local directory '/home/pilot-pjt/mahout-data/recommendation/input'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
select hash(car_number ), hash(item), score from managed_smartcar_item_buylist_info
===================================
	실행
	# 위 QL은 "스마트카 차량용품 구매 이력" 테이블인 Managed_SmartCar_Item_BuyList_Info로부터 분석에 필요한 차량번호(car_number), 상품코드(item), 평가점수(score) 데이터를 조회해서 로컬 서버(server02)의 파일시스템 경로인 /home/pilot-pjt/mahout-data/recommendation/input 에 파일을 생성하는 QL이다.
	# car_number와 item 데이터는 머하웃의 추천기 라이브러리의 입력 데이터로 사용하기 위해 숫자 타밉(Long)로 형변환 했고, 하이브의 hash() 함수를 이용했다.

	# 추천기의 입력 데이터로 사용될 파일이 정상적으로 만들어졌는지 확인
	[server02]
	more /home/pilot-pjt/mahout-data/recommendation/input/*
	# 3개의 숫자 타입의 데이터가 콤마로 구분되어 차량번호, 상품코드, 평가점수 순으로 표시된것을 확인할수 있다.
	# "000000_0"라는 이름으로 생성

	# 앞서 생성산"000000_0" 파일을 머하웃 추천기의 입력 데이터로 사용하기 위해 HDFS에 /pilot-pjt/mahout/recommendation/input/ 경로를 생성하고 "000000_0" 파일을 저장한다.
	hdfs dfs -mkdir /pilot-pjt/mahout
	hdfs dfs -mkdir /pilot-pjt/mahout/recommendation
	hdfs dfs -mkdir /pilot-pjt/mahout/recommendation/input
	hdfs dfs -put /home/pilot-pjt/mahout-data/recommendation/input/* /pilot-pjt/mahout/recommendation/input/item_buylist.txt

	# /CH07/Mahout/2nd에 있는 머하웃 추천 분석기를 실행
	mahout recommenditembased -i /pilot-pjt/mahout/recommendation/input/item_buylist.txt -o /pilot-pjt/mahout/recommendation/output/ -s SIMILARITY_COOCCURRENCE -n 3
	# 사용된 매개변수와 옵션
	i: 추천 분석에 사용할 입력 데이터
	o: 추천 분석 결과가 출력될 경로
	s: 추천을 위한 유사도 알고리즘
	n: 추천할 아이템 개수

	# 마지막에 "Info driver.MahoutDriver: Program took 518406 ms (Minutes: 8.6401)"과 같은 메시지가 출력되면 추천 분석이 정상적으로 끝난 것이다.

	# 분석 결과가 저장된 HDFS의 /pilot-pjt/mahout/recommendation/output/에 있는 파일을 휴의 파일 브라우저로 열어서 확인한다.
	# 해당 경로에는 두 개의 파일인 "part-r-00000", "part-r-00001"이 생성돼 있을 것이다. 이 가운데 하나를 열어서 추천 결과를 확인한다.

	# 입력 데이터가 새로 구성됐거나 머하웃의 추천기 명령 옵션을 변경해서 재실행이 필요할 때는 앞서 수행한 추천 명령으로 생성된 결과 파일과 임시 파일들을 다음의 명령어로 삭제한 후 재실행해야 한다.
	hdfs dfs -rm -R -skipTrash /pilot-pjt/mahout/recommendation/output
	hdfs dfs -rm -R -skipTrash /user/root/temp


2. 스마트카 상태 정보 예측/분류
	- 데이터 마이닝의 분류에 사용될 데이터셋은 "스마트카 상태 정보"로 하이브의 Manage 영역에 Managed_SmartCar_Status_Info 라는 이름의 테이블에 약 200만 건의 데이터가 적재돼 있다.

	# 하이브를 이용해 트레이닝 데이터셋을 만드는 작업을 한다.
	# "스마트카 상태 정보" 데이터를 머하웃의 분류기의 입력 데이터로 사용하기 위해 하이브로 재구성한다.
	# /CH07/HiveQL/2nd/그림-7.62.hql 코드 실행
=============[내부코드]================
insert overwrite local directory '/home/pilot-pjt/mahout-data/classification/input'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
select
  sex, age, marriage, region, job, car_capacity, car_year, car_model,
  tire_fl, tire_fr, tire_bl, tire_br, light_fl, light_fr, light_bl, light_br,
  engine, break, battery,
  case when ((tire_fl_s  + tire_fr_s  + tire_bl_s  + tire_br_s  +
              light_fl_s + light_fr_s + light_bl_s + light_br_s +
              engine_s   + break_s    + battery_s  +
              car_capacity_s + car_year_s + car_model_s) < 6)
       then '비정상' else '정상'
  end as status
from (
  select
    sex, age, marriage, region, job, car_capacity, car_year, car_model,
    tire_fl, tire_fr, tire_bl, tire_br, light_fl, light_fr, light_bl, light_br,
    engine, break, battery,

    case
	 when (1500 > cast(car_capacity as int)) then -0.3
        when (2000 > cast(car_capacity as int)) then -0.2
        else -0.1
    end as car_capacity_s ,

    case
	when (2005 > cast(car_year as int)) then -0.3
       when (2010 > cast(car_year as int)) then -0.2
       else -0.1
    end as car_year_s ,

    case
	when ('B' = car_model) then -0.3
       when ('D' = car_model) then -0.3
       when ('F' = car_model) then -0.3
       when ('H' = car_model) then -0.3
       else 0.0
    end as car_model_s ,

    case
       when (10 > cast(tire_fl as int)) then 0.1
       when (20 > cast(tire_fl as int)) then 0.2
       when (40 > cast(tire_fl as int)) then 0.4
       else 0.5
    end as tire_fl_s ,

    case
       when (10 > cast(tire_fr as int)) then 0.1
       when (20 > cast(tire_fr as int)) then 0.2
       when (40 > cast(tire_fr as int)) then 0.4
       else 0.5
    end as tire_fr_s ,

    case
       when (10 > cast(tire_bl as int)) then 0.1
       when (20 > cast(tire_bl as int)) then 0.2
       when (40 > cast(tire_bl as int)) then 0.4
       else 0.5
    end as tire_bl_s ,

    case
       when (10 > cast(tire_br as int)) then 0.1
       when (20 > cast(tire_br as int)) then 0.2
       when (40 > cast(tire_br as int)) then 0.4
       else 0.5
    end as tire_br_s ,

    case when (cast(light_fl as int) = 2) then 0.0 else 0.5 end as light_fl_s ,
    case when (cast(light_fr as int) = 2) then 0.0 else 0.5 end as light_fr_s ,
    case when (cast(light_bl as int) = 2) then 0.0 else 0.5 end as light_bl_s ,
    case when (cast(light_br as int) = 2) then 0.0 else 0.5 end as light_br_s ,

    case
       when (engine = 'A') then 1.0
       when (engine = 'B') then 0.5
       when (engine = 'C') then 0.0
    end as engine_s ,

    case
       when (break = 'A') then 1.0
       when (break = 'B') then 0.5
       when (break = 'C') then 0.0
    end as break_s ,

    case
       when (20 > cast(battery as int)) then 0.2
       when (40 > cast(battery as int)) then 0.4
       when (60 > cast(battery as int)) then 0.6
       else 1.0
    end as battery_s

  from managed_smartcar_status_info ) T1

=============================================
	실행
	# 위 하이브 QL은 "스마트카 상태 정보" 데이터로부터 예측 변수(차량용량, 차량연식, 차량모델, 타이어, 라이트, 엔진, 브레이크, 배터리)들의 상태값으로 주관적인 기준으로 스코어링했다.
	# 예측변수에는 보정치 변수(마이너스값)와 가중치 변수(플러스값)가 있고, 이 모든 예측변수의 값을 합산해서 "6" 미만인 경우 "비정상"을 "6" 이상인 경우에 "정상"인 값을 가지는 목표변수를 정의했다.
	# 하이브 QL의 실행 결과는 server02의 /home/pilot-pjt/mahout-data/classification/input 로컬 디렉터리에 생성된다.
	# 위 하이브 QL을 실행하면 에디터 상에서 다음과 같은 메시지가 연속적으로 표시된다.
	"'ascii' codec can't encode characters in position 288-289: ordinal not in range"   # 한글이 포함된 쿼리를 수행할 경우 중간 인코딩 변환에서 나타나는 오류로서 무시해도 좋다.

	# 예측변수와 목표변수 값이 들어간 "스마트카 상태 정보" 입력 데이터셋이 정상적으로 만들어졌는지 확인한다.
	[server02]
	more /home/pilot-pjt/mahout-data/classification/input/*

	# 파일럿 환경에 따라 "000000_0"과 "000001_0" 두 파일보다 더 만들어 질수 있다.

	# 분류기의 트레이닝 데이터셋을 만들기 위해 두개의 파일을 리눅스의 cat 명령을 이용해 하나의 파일로 합쳐 classification_dataset.txt 라는 이름의 파일을 만든다.
	cd /home/pilot-pjt/mahout-data/classification/input
	cat 000000_0 000001_0 > classification_dataset.txt
	# 앞선 cat 명령에서 머지할 파일이 없을때, 에러 메세지가 뜨는데 이는 무시해도 좋다.

	# 머하웃 분류기의 입력 데이터로 사용하기 위해 HDFS의 /pilot-pjt/mahout/classification/input/ 경로를 생성하고 classification_dataset.txt 파일을 저장한다.
	hdfs dfs -mkdir /pilot-pjt/mahout/classification
	hdfs dfs -mkdir /pilot-pjt/mahout/classification/input
	hdfs dfs -put /home/pilot-pjt/mahout-data/classification/input/classification_dataset.txt /pilot-pjt/mahout/classification/input

	# 머하웃 분류기의 머신러닝에서 사용할 디스크립터(Descriptor) 파일을 생성
	# 디스크립터 파일은 학습할 데이터의 형식을 정의하는 파일로서 앞서 생성한 입력 데이터셋(classification_dataset.txt) 파일을 이용한다.
	hadoop jar /opt/cloudera/parcels/CDH/lib/mahout/mahout-examples-job.jar org.apache.mahout.classifier.df.tools.Describe -p /pilot-pjt/mahout/classification/input/classification_dataset.txt -f /pilot-pjt/mahout/classification/output/descriptor/smartcar-status.descriptor -d I I I I I N N C N N N N N N N N C C N L
	# 문제없이 실행됐다면 "storing the dataset description" 이라는 메시지가 표시된다.

	# 위 디스크립터 생성 명령에 사용된 매개변수 및 옵션
	p: 트레이닝에 사용할 데이터셋 경로
	f: 디스크립터 파일이 생성될 경로
	d: 트레이닝에 사용할 데이터셋의 형식 지정(I: 부시, N:숫자, C: 분류문자열, L:목표변수)

	# 각 필드에 지정된 -d 옵션값
	성별(C), 나이(N), 결혼 여부(C), 거주지(C), 직업(C), 차량CC(N), 차량 연식(N), 차량모델(C), 타이어 상태(N,N,N,N), 라이트 상태(N,N,N,N), 엔진상태(C), 브레이크상태(C), 배터리상태(N)

	# 휴의 파일 브라우저를 통해 pilot-pjt/mahout/classification/output/descriptor에 디스크립터 파일인 smartcar-status.descriptor 가 정상적으로 만들어 졌는지 확인한다.

	# 머하웃 분류기의 입력 데이터셋(classification_dataset.txt)을 머신러닝에 활용하기 위해서는 학습 데이터셋과 테스트 데이터셋이 필요한데, 입력 데이터셋에서 7:3 또는 8:2 비율로 분할 생성한다.
	# 머하웃의 split 명령을 실행
	mahout split -i /pilot-pjt/mahout/classification/input --trainingOutput /pilot-pjt/mahout/classification/input/training --testOutput /pilot-pjt/mahout/classification/input/testing --randomSelectionPct 30 --overwrite --method sequential
	# 사용된 매개변수 및 옵션
	i: 분할(Split)시킬 원천 파일 경로
	trainingOutput: 학습 데이터셋 생성 경로
	testOutput: 테스트 데이터셋 생성 경로
	randomSelectionPct: 학습 및 테스트 데이터셋의 비율 지정
	overwrite: 결과 덮어 쓰기
	method: 분할 방식을 지정

	# 정상적으로 실행돼면 다음과 같은 메시지 출력
	Program took 13690 ms (Minutes: 0.2281..........)

	# 휴의 파일 브라우저를 통해 pilot-pjt/mahout/classification/input 경로 안에 "testing", "training" 디렉터리가 만들어졌는지 확인하고 각 디렉터리 안에 classification_dataset.txt 파일이 생성됐는지 확인

	# 스마트카의 상태 정보 예측 모델을 만들기 위해 트레이닝 데이터셋과 분류 알고리즘 중 하나인 랜덤 포레스트를 이용해 분류기 모델을 트레이닝시킨다.
	# 입력데이터로는 앞서 만들었던 디스크립터 파일과 트레이닝 파일이 사용된다.
	hadoop jar /opt/cloudera/parcels/CDH/lib/mahout/mahout-examples-job.jar org.apache.mahout.classifier.df.mapreduce.BuildForest -Dmapred.max.split.size=1874231 -d /pilot-pjt/mahout/classification/input/training/classification_dataset.txt -ds /pilot-pjt/mahout/classification/output/descriptor/smartcar-status.descriptor -sl 7 -p -t 100 -o /pilot-pjt/mahout/classification/output/model
	# 사용된 매개변수 및 옵션
	Dmapred.max.split.size: 맵 생성 단위를 파일 크기로 지정
	d: 트레이닝 데이터셋 경로
	ds: 디스크립터 파일 경로
	sl: 의사결정 트리(Decision Tree)에 사용할 변수 개수를 지정
	p: 트레이닝 시 데이터를 분할해서 실행
	t: 모델에 포함될 의사결정 트리 개수를 지정
	o: 분류기 모델 생성 경로
	# 정상적으로 실행되면 아래메시지 확인 가능
	INFO mapreduce.BuildForest: Build Time: 0h 8m 42s 939
	INFO mapreduce.BuildForest: Forest num Nodes: 19474
	INFO mapreduce.BuildForest: Forest mean num Nodes: 194
	INFO mapreduce.BuildForest: Forest mean max Depth: 11
	INFO mapreduce.BuildForest: Storing the forest in: /pilot-pjt/mahout/classification/output/model/forest.seq
	# 스마트카 상태 정보 문류 모델이 정상적으로 만들어지면 빌드시간(Build Time)과 포레스트 노드(Forest Node) 개수, 랜덤 포레스트 모델 출력 위치를 표시하게 되며, 휴의 파일 브라우저를 통해 생성된 모델의 위치를 /pilot-pjt/mahout/classification/output/model 에서 확인할수 있다. forest.seq 파일 확인

	# 랜덤 포레스트의 분류기 모델을 평가해 본다. pilot-pjt/mahout/classification/input/testing 디렉터리에 만들어 놓은 파일을 입력 데이터로 해서 아래 명령을 실행한다.
	hadoop jar /opt/cloudera/parcels/CDH/lib/mahout/mahout-examples-job.jar org.apache.mahout.classifier.df.mapreduce.TestForest -i /pilot-pjt/mahout/classification/input/testing/classification_dataset.txt -ds /pilot-pjt/mahout/classification/output/descriptor/smartcar-status.descriptor -m /pilot-pjt/mahout/classification/output/model -a -mr -o /pilot-pjt/mahout/classification/output/predictions
	# 사용된 매개변수 및 옵션
	i: 평가(테스트) 데이터셋 파일경로
	ds: 디스크립터 파일 경로
	m: 분류기 모델 파일 경로
	a: 평가결과 표준 출력
	mr: 맵리듀스 기반 평가 동작
	o: 평가 결과 출력 경로
	# 평가결과(Summary)를 확인해보면 테스트 데이터를 이용한 분류 성공율이 97.5217%, 실패율이 2.4783%로 나왔다.
	# Confusion Matrix는 비정상(a), 정상(b)의 분류 케이스들을 매트릭스로 표현한 것이다.
	# pilot-pjt/mahout/classification/output/predictions 에는 분류 모델의 예측 결과값이 출력돼있다.

	# Classify 프로그램을 새로운 "스마트카 상태 정보" 데이터셋에 적용해 차량의 상태가 정상 또는 비정상인지를 분류 및 예측한다.
	# 아래는 Classify 자바 애플리케이션 예제다. 전체 프로그램은 /workspace/bigdata.smartcar.mahout 경로에 있다.
==========[예제]===========
package com.wikibook.bigdata.smartcar.mahout;

import java.io.IOException;
import java.util.Random;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.mahout.common.RandomUtils;
import org.apache.mahout.classifier.df.DecisionForest;
import org.apache.mahout.classifier.df.data.Dataset;
import org.apache.mahout.classifier.df.data.Instance;

import org.apache.mahout.math.DenseVector;

public class ClassifySmartCarStatus {

  public static void main(String[] args) throws IOException {
    Configuration config = new Configuration();

    DecisionForest classifyModel = DecisionForest.load(
    		config, new Path("./classification/model/forest.seq"));
    Dataset dataset = Dataset.load(
    		config, new Path("./classification/descriptor/smartcar-status.descriptor"));

    int i=0;

    DenseVector car_vector = new DenseVector(14);
    car_vector.set(0, Integer.parseInt(args[i++])); 	//car_capacity
    car_vector.set(1, Integer.parseInt(args[i++])); 	//car_year
    car_vector.set(2, dataset.valueOf(2, args[i++])); 	//car_model
    car_vector.set(3, Integer.parseInt(args[i++])); 	//타이어 1
    car_vector.set(4, Integer.parseInt(args[i++])); 	//타이어 2
    car_vector.set(5, Integer.parseInt(args[i++])); 	//타이어 3
    car_vector.set(6, Integer.parseInt(args[i++])); 	//타이어 4
    car_vector.set(7, Integer.parseInt(args[i++]));  	//라이트 1
    car_vector.set(8, Integer.parseInt(args[i++]));  	//라이트 2
    car_vector.set(9, Integer.parseInt(args[i++]));  	//라이트 3
    car_vector.set(10, Integer.parseInt(args[i++])); 	//라이트 4
    car_vector.set(11, dataset.valueOf(11, args[i++])); //엔진
    car_vector.set(12, dataset.valueOf(12, args[i++])); //브레이크
    car_vector.set(13, Integer.parseInt(args[i++])); 	//베터리

    Instance instance = new Instance(car_vector);
    Random rNum = RandomUtils.getRandom();

    double prediction = classifyModel.classify(dataset, rNum, instance);

    System.out.println(" SmartCar Status Prediction :"+dataset.getLabelString(prediction));
  }
}
=========================================================
	# 위 예제를 직접 실행해서 분류기의 예측값이 정상적으로 작동하는지 확인한다.
	# 미리 빌드되어있는 파일 bigdata.smartcar.mahout-1.0.jar 파일은 server02에 업로드한다.
	머하웃 작업 경로: sftp://server02.hadoop.com/home/pilot-pjt/mahout-data
	소스 경로: CH07/bigdata.smartcar.mahout-1.0.jar

	# 분류기 모델을 적용하기 위해서는 "Descriptor" 파일과 "Classify" 파일이 필요하다. 앞서 HDFS에 생성했던 디스크립터 파일을 로컬 디렉터리로 가져온다.
	# HDFS 상에 /pilot-pjt/mahout/classification/output/descriptor/ 에 위치한 smartcar-status.descriptor 파일이다.
	[server02]
	mkdir /home/pilot-pjt/mahout-data/classification/descriptor
	cd /home/pilot-pjt/mahout-data/classification/descriptor
	hdfs dfs -get /pilot-pjt/mahout/classification/output/descriptor/smartcar-status.descriptor

	# "Classify" 파일을 로컬 디렉터리로 가져온다. HDFS의 /pilot-pjt/mahout/classification/output/model/ 에 위치한 forest.seq 파일이다.
	mkdir /home/pilot-pjt/mahout-data/classification/model
	cd /home/pilot-pjt/mahout-data/classification/model
	hdfs dfs -get /pilot-pjt/mahout/classification/output/model/forest.seq

	# 스마트카 상태 정보의 분류기 모델이 정상적으로 작동하는지 ClassifySmartCarStatus.java를 실행하면서 14개의 인자값 "차량 용량", "차량 연식", "차량 모델", "타이어 상태 x 4", "라이트 상태 x 4", "엔진", "브레이크", "배터리" 를 순서대로 입력한다.
	cd /home/pilot-pjt/mahout-data
	java -cp bigdata.smartcar.mahout-1.0.jar com.wikibook.bigdata.smartcar.mahout.ClassifySmartCarStatus 2000 2000 A 80 80 80 80 1 1 1 1 A B 50
	# 임의로 입력했던 스마트카 상태 정보는 분류기 모델이 "비정상"으로 분류 했다.
	# 인자값 중 맨 마지막에 있는 배터리의 상태값을 "50" -> "80"으로 바꿔서 실행해 본다.
	java -cp bigdata.smartcar.mahout-1.0.jar com.wikibook.bigdata.smartcar.mahout.ClassifySmartCarStatus 2000 2000 A 80 80 80 80 1 1 1 1 A B 80
	# 인자값을 다양하게 바꿔가면서 분류기의 예측 결과를 테스트한다.
	# 실제 분류기를 적용할 때는 지속적인 학습을 거쳐 Classify 프로그램의 정확도를 높이는 과정이 중요하다.


3. 스마트카 고객 정보 분석
	- 군집 분석 진행
	- 사용할 데이터셋은 "스마트카 고객 마스터 정보"로 하이브의 External 영역에 SmartCar_Master 테이블이다.
	- 데이터셋에는 스마트카의 차량번호, 차량용량, 차량모델 정보와 스마트카 사용자의 성별, 나이, 결혼여부, 직업, 거주지역 정보들이 있다.
	- 군집분석은 이러한 속성 정보를 벡터화하고 유사도 및 거리를 계산해 데이터의 새로운 군집을 발견하는 마이닝 기법이다.

	# 하이브를 이용해 "스마트카 마스터 정보" 데이터셋을 조회해서 로컬 디스크에 저장한다. 휴의 하이브 에디터에서 다음의 경로의 QL을 실행한다.
	# /CH07/HiveQL/2nd/그림-7.75.hql
	휴 브라우저 -> Hive Editor
=============[내부코드]=============
insert overwrite local directory '/home/pilot-pjt/mahout-data/clustering/input'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ' '
select
  car_number,
  case
      when (car_capacity < 2000) then '소형'
      when (car_capacity < 3000) then '중형'
      when (car_capacity < 4000) then '대형'
  end as car_capacity,
  case
      when ((2016-car_year) <= 4)  then 'NEW'
      when ((2016-car_year) <= 8)  then 'NORMAL'
      else 'OLD'
  end as car_year ,
  car_model,
  sex as owner_sex,
  floor (cast(age as int) * 0.1 ) * 10 as owner_age,
  marriage as owner_marriage,
  job as owner_job,
  region as owner_region
from smartcar_master
=================================
	실행
	# 하이브 QL은 "스마트카 마스터 정보" 테이블로부터 차량 번호, 차량 용량, 차량 연도, 차량 모델, 운전자의 성별, 나이, 결혼여부, 직업, 거주지 데이터만 선택적으로 조회한다.
	# 이때 편차가 큰 "차량 용량", "차량 연도", "운전자 나이"의 경우 범주화함으로써 군집의 정확도를 높일수 있다.
	# 수행결과는 server02의 /home/pilot-pjt/mahout-data/clustering/input 로컬 디렉터리에 생성된다.

	# 군집분석을 하기 위한 "스마트카 사용자 마스터" 데이터셋이 정상적으로 만들어졌는지 확인
	[server02]
	more /home/pilot-pjt/mahout-data/clustering/input/*
	# /home/pilot-pjt/mahout-data/clustering/input/ 으로 이동해 보면 "000000_0" 이라는 이름의 파일이 만들어져 있다

	# 머하웃 군집분석의 입력데이터로 사용하기 위해 HDFS 상에 /pilot-pjt/mahout/clustering/input/ 경로를 생성하고 앞서 생성한 데이터인 "000000_0" 파일 저장
	hdfs dfs -mkdir /pilot-pjt/mahout/clustering
	hdfs dfs -mkdir /pilot-pjt/mahout/clustering/input
	hdfs dfs -put /home/pilot-pjt/mahout-data/clustering/input/000000_0 /pilot-pjt/mahout/clustering/input

	# 고객 마스터를 군집분석하기 위한 데이터가 HDFS에 정상적으로 적재됐는지 휴의 파일브라우저로 확인
	휴 브라우저 파일 경로: /pilot-pjt/mahout/clustering/input/000000_0

	# 머하웃의 군집분석을 하기 위해서는 원천 파일이 시퀀스 파일이어야 한다.
	# HDFS에 적재한 고객 마스터 데이터인 "000000_0" 파일은 텍스트 파일 형식이므로 시퀀스 파일로 변환한다.
	# 시퀀스 파일은 키/값 형식의 바이너리 데이터셋으로 분산 환경에서 성능과 용량 면에서 효율성을 높인 데이터 포멧이다.
	# 이번 군집분석에서는 시퀀스 파일의 키를 차량 번호로 하고, 나머지 마스터(차량연도, 차량용량, 모델, 나이, 연령 등)를 값으로 구성하기 위해 간단한 시퀀스 파일 변환 프로그램인 "com.wikibook.bigdata.smartcar.mahout.TextToSequence"를 이용한다.
	# 소스 프로그램은 /workplace/Mahout project 에 있다.
	# TextToSequence 프로그램을 실행하기 위해 사전에 빌드해 놓은 bigdata.smartcar.mahout-1.0.jar파일을 server02 의 /home/pilot-pjt/mahout-data에 업로드 한다.
	머하웃 작업 경로: sftp://server02.hadoop.com/home/pilot-pjt/mahout-data
	소스 경로: /CH07/bigdata.smartcar.mahout-1.0.jar

	# 텍스트 형식의 "스마트카 사용자 마스터" 파일을 시퀀스 파일로 변환한다.
	# 변환대상은 앞서 HDFS에 저장해둔 /pilot-pjt/mahout/clustering/input/000000_0 파일이고, 변환 결과는 HDFS의 /pilot-pjt/mahout/clustering/output/seq 에 생성된다.
	hadoop jar /home/pilot-pjt/mahout-data/bigdata.smartcar.mahout-1.0.jar com.wikibook.bigdata.smartcar.mahout.TextToSequence /pilot-pjt/mahout/clustering/input/000000_0 /pilot-pjt/mahout/clustering/output/seq
	# 정상적으로 변환될 경우 "Map-Reduce Framework... " 형식의 메세지 확인가능

	# 변환된 시퀀스 파일을 휴의 파일 브라우저를 통해 확인해보자. 해당 경로에 part-m-00000이라는 이름의 시퀀스 파일이 생성됐을것이다.
	# 경로: /pilog-pjt/mahout/clustering/output/seq/part-m-00000

	# 시퀀스 파일의 내용을 확인하기 위해 다음의 HDFS 명령을 이용할 수 있다
	hdfs dfs -text /pilot-pjt/mahout/clustering/output/seq/part-m-00000
	# text 옵션을 지정하면 시퀀스 파일의 내용을 텍스트 형식으로 확인할 수 있다.

	# 시퀀스 파일을 로우별(차량번호)로 n-gram 기반의 TF(Team Frequency) 가중치가 반영된 벡터 데이터로 변환한다.
	# n-gram의 벡터 모델은 단어의 분류와 빈도 수를 측정하는 알고리즘 정도로 이해
	# 여기서는 차량번호별 각 항목의 단어를 분리해 벡터화하기 위해 사용한다.
	# 다음 명령을 실행해 스마트카 마스터 데이터를 다차원의 공간 벡터로 변환해 HDFS의 /pilot-pjt/mahout/clustering/output/vec 에 생성한다.
	mahout seq2sparse -i /pilot-pjt/mahout/clustering/output/seq -o /pilot-pjt/mahout/clustering/output/vec -wt tf -s 5 -md 3 -ng 2 -x 85 --namedVector
	# 적용된 옵션에 대한 설명
	wt: 단어 빈도 가중치 방식
	md: 최소 문서 출현 횟수
	ng: ngrams 최댓값
	namedVector: 네임벡터 데이터 생성
	# 맵리듀스 잡이 여러 차례 실행되고 벡터 데이터가 정상적으로 만들어지면 휴의 파일 브라우저를 통해 pilot-pjt/mahout/clustering/output/vec 경로에 df-count, tf-vectors 등의 디렉터리와 파일이 생성돼 있음을 확인할 수 있다.

	# Canopy 군집분석으로 최적의 군집 개수를 파악하기 위해서는 센트로이드로부터 거리를 나타내는 t1, t2 옵션을 바꿔가며 반복적인 군집분석을 수행해야 한다.
	# 다음과 같은 명령으로 첫 번째 Canopy 군집분석을 실행해 본다.
	# 캐노피 클러스터 수행
	mahout canopy -i /pilot-pjt/mahout/clustering/output/vec/tf-vectors/ -o /pilot-pjt/mahout/clustering/canopy/out -dm org.apache.mahout.common.distance.SquaredEuclideanDistanceMeasure -t1 50 -t2 45 -ow
	# 적용된 옵션에 대한 설명
	i: 벡터 파일 경로
	o: 출력 결과 경로
	dm: 군집 거리 측정 알고리즘
	t1: 거리값 1
	t2: 거리값 2
	# Canopy 군집분석이 정상적으로 수행되면 HDFS의 /pilot-pjt/mahout/clustering/canopy/out/ 경로에 clusters-xx-final 이라는 디렉터리가 만들어지고 그 안에 결과 파일이 생성돼 있다
	# 첫 번째 실행에서 "t1=50, t2=45"로 설정하고, 유사도 거리 측정을 위해 SquaredEuclideanDistanceMeasure를 사용했다.

	# 캐노피 클러스터 결과 확인
	mahout clusterdump -i /pilot-pjt/mahout/clustering/canopy/out/clusters-*-final

	# "INFO clustering.ClusterDumper: Wrote 1 clusters"
	# Canopy 군집분석 결과 1개의 군집이 만들어 졌다. 2,600명의 스마트카 사용자 마스터 데이터가 1개의 군집으로 분석됐다면 t1, t2의 각 초기 거리값을 너무 크게 잡은 것이다.
	# 캐노피 클러스터 수행
	mahout canopy -i /pilot-pjt/mahout/clustering/output/vec/tf-vectors/ -o /pilot-pjt/mahout/clustering/canopy/out -dm org.apache.mahout.common.distance.SquaredEuclideanDistanceMeasure -t1 10 -t2 8 -ow

	# 캐노피 클러스터 결과 확인
	mahout clusterdump -i /pilot-pjt/mahout/clustering/canopy/out/clusters-*-final

	# 이번에는 819개의 클러스터가 만들어진 것을 확인할수 있다.
	# 전체 스마트카 사용자 2,600명을 대상으로 819개의 군집으로 생성했다는 것은 t1, t2의 각 거리값을 너무 작게 잡았다고 볼수 있다.

	# 마지막으로 t1, t2의 값을 각각 12, 10으로 설정하고 Canopy 군집분석을 실행한다.
	# 캐노피 클러스터 수행
	mahout canopy -i /pilot-pjt/mahout/clustering/output/vec/tf-vectors/ -o /pilot-pjt/mahout/clustering/canopy/out -dm org.apache.mahout.common.distance.SquaredEuclideanDistanceMeasure -t1 12 -t2 10 -ow

	# 캐노피 클러스터 결과 확인
	mahout clusterdump -i /pilot-pjt/mahout/clustering/canopy/out/clusters-*-final

	# 148개의 군집이 생성됐다. 2,600 명의 고객을 대상으로 148개의 군집은 적절해 보인다. 적절하다는 기준은 다소 주관적이다. 분석 요건과 데이터의 성격에 따라 적절한 군집의 개수를 판단한다.

	# K_Means 군집분석 시작.
	# K_Means 군집분석을 실행할 때는 초기 군집값(K)이 필요한데, 이때 K 값으로 앞서 수행한 Canopy 군집분석에서 나온 클러스터 수(148)를 활용한다.
	# K-Means의 K 값을 "148" 보다 좀 더 크게 해서 "200"으로 설정하고 다음 명령을 실행한다.
	mahout kmeans -i /pilot-pjt/mahout/clustering/output/vec/tf-vectors -c /pilot-pjt/mahout/clustering/kmeans/output/cluster -o /pilot-pjt/mahout/clustering/kmeans/output -dm org.apache.mahout.common.distance.SquaredEuclideanDistanceMeasure -cd 1.0 -k 200 -x 20 -cl -xm mapreduce -ow
	# "Map-Reduce Framework..." 메시지 확인
	# 적용된 옵션에 대한 설명
	i: 벡터 파일 경로
	c: 초기 클러스터 데이터 생성 경로
	o: 출력 결과 생성 경로
	dm: 군집 거리 측정 알고리즘
	cd: 수렴할 임계값 지정
	k: 군집 수 지정
	x: 최대 반복 수행 횟수
	xm: 실행 방식 선정
	# 아래의 종료 메시지 확인
	Map-Reduce Framework
		Map input records=2600
		Map output records=2600
		Input split bytes=159
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=116
		CPU time spent (ms)=4070
		Physical memory (bytes) snapshot=136269824
		Virtual memory (bytes) snapshot=1499791360
		Total committed heap usage (bytes)=78774272
	File Input Format Counters
		Bytes Read=422007
	File Output Format Counters
		Bytes Written=520916
	INFO driver.MahoutDriver: Program took 354448 ms (Minutes: 5.907466666666667)

	# K-Means 군집 분석 결과를 확인.
	# 다음 명령을 실행하면 K-Means 군집분석 결과가 로컬 디스크의 /home/pilot-pjt/mahout-data/clustering/output/clusterdump_result.txt 에 생성된다.
	#K-Means 결과 파일 생성
	mahout clusterdump -i /pilot-pjt/mahout/clustering/kmeans/output/clusters-*-final -p /pilot-pjt/mahout/clustering/kmeans/output/clustersPoints -o /home/pilot-pjt/mahout-data/clustering/output/clusterdump_result.txt

	#K-Means 결과 파일 확인
	more /home/pilot-pjt/mahout-data/clustering/output/clusterdump_result.txt

	# "VL-1418"은 클러스터 이름이다.
	# "n=11"은 클러스터에 포함된 대상의 개수
	# "c={1:0.92...}"는 클러스터의 중심값, "r={1:0.287,...}"는 클러스터의 반경
	# clusterdump_result.txt 파일에는 입력값으로 사용했던 "군집수(K)=200" 만큼의 군집 데이터가 만들어져 있을 것이다.
	# 군집의 결과를 확인하기 위해 특정 클러스터 하나를 임의로 선정한다. 파일럿 환경마다 클러스터 이름이 다르다.
	클러스터 이름: VL-1418

	# "VL-1418" 군집을 형성하는 주요 키워드는 무엇인지 확인해 보자. 다음 머하웃의 clusterdump 명령을 실행하면 server02의 /home/pilot-pjt/mahout-data/clustering/output 에 clusterdump_result2.txt 파일이 생성된다.
	# 키워드 추출 명령
	mahout clusterdump -dt sequencefile -d /pilot-pjt/mahout/clustering/output/vec/dictionary.file-* -i /pilot-pjt/mahout/clustering/kmeans/output/clusters-*-final -o /home/pilot-pjt/mahout-data/clustering/output/clusterdump_result2.txt -b 10 -n 10 -dm org.apache.mahout.common.distance.SquaredEuclideanDistanceMeasure

	# 추출된 키워드 확인
	grep -11 ":VL-1418" /home/pilot-pjt/mahout-data/clustering/output/clusterdump_result2.txt
	# "VL-1418" 군집은 "여(성별)", "기혼(결혼여부)", "b(차량모델)", "old(차량년도)" 순으로 군집 형성에 영향을 주었다

	# 머하웃의 seqdumper 명령을 실행해 "VL-1418" 군집에 포함된 차량을 확인해 보겠다.
	# 다음 명령을 실행해 로컬의 /home/pilot-pjt/mahout-data/clustering/output 경로에 seqdumper_result.txt 파일을 생성한다.
	# 군집에 포함된 차량 추출
	mahout seqdumper -i /pilot-pjt/mahout/clustering/kmeans/output/clusteredPoints/ -o /home/pilot-pjt/mahout-data/clustering/output/seqdumper_result.txt

	# 군집에 포함된 차량 확인
	grep "Key: 1418" /home/pilot-pjt/mahout-data/clustering/output/seqdumper_result.txt
	# "VL-1418"에 포함된 11대의 스마트카 차량번호가 있다. 이 11대의 스마트카 마스터 정보가 유사성이 있고 같은 군집에 형성된 차량임을 알수 있다.

	# "VL-1418" 군집에 포함된 11대의 스마트카 마스터 정보를 휴 또는 임팔라 에디터에서 작성하고 실행한다.
	# 조회 조건인 car_number 정보로는 독자의 파일럿 환경에서 실행했던 직전 [군집에 포함된 차량 추출]명령어 실행 결과로 출력된 차량번호들을 입력해야 한다.
	휴 -> 임팔라 에디터 ->
===========[내부 코드]=============
select * from smartcar_master

where car_number

in ('C0029', 'C0052', 'E0050', 'H0082', 'M0085', '00019', 'P0074', '50062', 'V0055', 'Y0035', 'Z0001')
=========================================
	실행

	# "VL-1418" 군집의 결과를 통해 다양한 인사이트를 도출할 수 있다.
	1. VL-1418 그룹의 고객군은 50대 기혼 여성들로 스마트카 B 모델을 선호한다.
	2. VL-1418 그룹에 속한 고객군의 차량은 중, 대형 차량으로 소득 수준이 높은 것으로 추측된다.
	3. VL-1418 그룹과 유사한 고객군에는 스마트카 B 모델 또는 유사 모델로 타깃 마켓팅한다.

































