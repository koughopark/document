1. 제플린을 이용한 실시간 분석
	# 스마트카의 운행지역에 대한 실시간 분석과 데이터 시각화를 위해 제플린을 이용
	# 제플린 NoteBook에서 스파크 프로그래밍을 하고, 분석 명령을 하둡 클러스터에 전달해 실행 결과를 제플린의 웹 UI로 전달받아 시각화한다.

	# 제플린을 이용한 운행 지역 분석
	# 스마트카가 운행한 지역들의 평균 속도를 구하고, 평균 속도가 높은 순서대로 출력

	# 제플린이 구동됐는지 확인, 아직 구동전이면 아래 명령으로 제플린 인스턴스를 재구동
	[server02]
	cd /home/pilot-pjt/zeppelin-0.6.2-bin-all/bin
	./zeppelin-daemon.sh start

	# "OK"가 표시되는것을 확인
	./zeppelin-daemon.sh status

	# 크롬 브라우저에서 제플린 환경 접속 확인
	http://server02.hadoop.com:8081/

	# 제플린 Notebook 생성
	제플린 브라우저 -> 상단 메뉴 Notebook 클릭 -> Create new note 클릭 ->
	Note Name: SmartCar-Project	-> Create Note 클릭

	# HDFS 명령어를 실행해 분석할 "스마트카 운전자의 운행" 파일을 확인
	# 노트북에서 셸 명령이 가능하도록 바인딩
	# 노트에 "%md"가 표시돼 있으면 이를 지우고 "%sh"로 입력한후 엔터키를 누른다

	# 하이브의 Managed 영역에 만들어져 있는 "스마트카 운전자 운행 정보" 파일을 확인하기 위해 다음 명령어를 입력하고 오른쪽의 [실행]버튼을 클릭한다.
	# biz_date 정보는 독자의 파일럿 상황에 맞는 날짜로 입력한다.
	[내부 코드]
%sh
hdfs dfs -cat /user/hive/warehouse/managed_smartcar_drive_info/biz_date=20180905/* | head
	===========
	실행

	# 노트북의 인터프리터를 스파크로 바인딩
	%spark
	실행

	# 스파크의 스칼라 코드 입력
	# /CH07/Zeppelin-SparkSQL/2nd 에서 "그림-7.43" 파일 코드 입력
	# biz_date 는 각자의 파일럿 환경에 맞게 입력
========[내부 코드]===============
val driveData = sc.textFile("hdfs://server01.hadoop.com:8020/user/hive/warehouse/managed_smartcar_drive_info/biz_date=20180905/*")
case class DriveInfo(car_num: String,        sex: String,            age: String,            marriage: String,
                     region: String,         job: String,            car_capacity: String,   car_year: String,
                     car_model: String,      speed_pedal: String,    break_pedal: String,    steer_angle: String,
                     direct_light: String,   speed: String,          area_num: String,       date: String)

val drive = driveData.map(sd=>sd.split(",")).map(
                sd=>DriveInfo(sd(0).toString, sd(1).toString, sd(2).toString, sd(3).toString,
                              sd(4).toString, sd(5).toString, sd(6).toString, sd(7).toString,
                              sd(8).toString, sd(9).toString, sd(10).toString,sd(11).toString,
                              sd(12).toString,sd(13).toString,sd(14).toString,sd(15).toString
        )
)

drive.toDF().registerTempTable("DriveInfo")
======================================
	실행

	# 정상적으로 실행되면 아래 메세지 확인 가능
driveData: org.apache.spark.rdd.RDD[String] = hdfs://server01.hadoop.com:8020/user/hive/warehouse/managed_smartcar_drive_info/biz_date=20180905/* MapPartitionsRDD[1] at textFile at <console>:27
defined class DriveInfo
drive: org.apache.spark.rdd.RDD[DriveInfo] = MapPartitionsRDD[3] at map at <console>:31

	# 예제를 확인해보면 로드한 데이터셋을 스파크-SQL로 분석하기 위해 임시 테이블인 DriveInfo를 생성한것을 확인할수 있다.
	# 그럼 스파크-SQL을 실행하기 위해 노트북의 인터프리터를 이제 스파크-SQL로 변경한다.
	# 새 노트에 "%spark.sql"을 입력하고 엔터
	# 스마트카가 운행한 지역의 평균 속도를 구하고, 평균 속도가 높은 순서대로 출력하는 쿼리
	========[내부코드]=========
%spark.sql
select T1.area_num, T1.avg_speed
from  (select area_num, avg(speed) as avg_speed
       from DriveInfo
       group by area_num
       ) T1
order by T1.avg_speed desc
==============================
	실행

	# 기존 스파크-SQL의 조건 변수를 설정해서 좀더 효율적인 분석을 할수 있다.
	# Having 절을 추가해서 평균 속도에 대한 동적 변수인 AvgSpeed를 정의하고 실행
	========[내부코드]=========
select T1.area_num, T1.avg_speed
from  (select area_num, avg(speed) as avg_speed
       from DriveInfo
       group by area_num having avg_speed >= ${AvgSpeed=60}
       ) T1
order by T1.avg_speed desc
==============================
	실행

	# 제플린의 스케줄러 기능을 이용해 앞서 스파크 스크립트로 만든 노트를 일정 시간이 되면 자동으로 실행되도록 설정한다.
	제플린 브라우저 -> 상단의 시계모양 아이콘 클릭 -> Cron 표현식 형식의 창 출력
	-> Preset None 라인에서 1m 클릭 or Cron expression 에서 직접 입력   # 1분 단위로 작업이 실행
	# 1분 기다리면 자동으로 노트에 작성된 스크립트가 실행되고 결과가 출력된다.

	# 노트북에서 작성한 작업 내용은 삭제하지 않는 이상 계속 유지된다.